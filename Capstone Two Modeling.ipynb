{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# War in Ukraine: Modeling Twitter Data\n",
    "\n",
    "## Background\n",
    "\n",
    "On February 24th, 2022, Russia invaded Ukraine after months of military preparation around the borders. Putin insists on calling it a \"special military operation\" and punishes anyone who calls it a \"war\" or defies the Russian state's completely false and fabricated narrative, which claims that Russian military is saving ethnic Russians and Ukrainians from \"Nazi\" officials in Ukraine.\n",
    "\n",
    "Everyone speculated that Russian forces would bulldoze over Ukrainian forces akin to a \"Blitzkrieg\" operation. Contrary to many's belief, especially Putin's, Ukrainian forces, with a significant backing from the West, has put up a tough resistance, even pushing out Russian forces in some major cities after weeks of battling as of early May. \n",
    "\n",
    "This war is having ripple effects across the world. In trade, grain, a major export product of Ukraine and Russia, has gone up in price. Energy market has experienced a shake-up since the war began and is expecting a very uncertain future as the West slowly weans off of Russian oil and gas. In geopolitics, Russia is almost cerntainly going to be more isolated, which is pushing it to harden its alliance with China, which in turn is navigating with care as to not violate the sanctions imposed on Russia. Countries that have not been part of NATO, like Finland and Sweden, are now more eager to join the alliance aghast by the Russian aggression. As such, we are witnessing a fundamentally changing world due to Russia's invasion of Ukraine.\n",
    "\n",
    "The war is going to have huge impacts around the world, perhaps even ending the globalized era as we know it. It is imperative that we capture the massive amounts of data being put out as a result of this war and extract insights for future generations.\n",
    "\n",
    "## Goals\n",
    "\n",
    "In an effort to make a record of this historical atrocity, I am initiating a project that monitors the progression of Twitterverse surrounding the war in Ukraine. On a regular basis, this project aims to conduct natural language processing tasks (e.g., wordcloud, sentiment analysis, topic modeling, etc.) to help the public keep track of what aspects of the war people are discussing on Twitter and how they feel about them, essentially tracking the Twitter users' changing views on the war. \n",
    "\n",
    "As a start, this project will **perform sentiment analysis on each month** since the beginning of the war separately. One this reason to this approach is to avoid exceeding the memory cap on Kaggle notebook. Thus, if necessary, a particular month's data could be divided into batches if for some reason there is an inordinate amount of data. In addition to EDA and sentiment analysis, the project will **perform topic modeling** to capture what the hottest issues are to the people on Twitter.\n",
    "\n",
    "Once several notebooks are out, say up to April data, **the project will try to automate the entire process** so that the analyses get updated on a regular basis. The goal is to have multiple dashboards each of which visualizes the Twitterverse in a specific time window (e.g., month) for the public to understand easily. The outputs may be displayed on a web application, though this is further down the line of development.\n",
    "\n",
    "\n",
    "\n",
    "## February Data - Exploratory Data Analysis and Data Cleaning\n",
    "\n",
    "In this notebook we will do data-cleaning/wrangling and exploratory data analysis of the February tweets about the war in Ukraine.\n",
    "\n",
    "EDA:\n",
    "- Wordcloud\n",
    "- Tweets per day\n",
    "- Top users by tweet frequency\n",
    "- Tweet length distribution\n",
    "\n",
    "Data Cleaning:\n",
    "- Read in February tweets\n",
    "- Check the data types\n",
    "- Check for missing data\n",
    "- Check for duplicates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-27T19:03:02.147091Z",
     "iopub.status.busy": "2022-05-27T19:03:02.146808Z",
     "iopub.status.idle": "2022-05-27T19:03:37.508049Z",
     "shell.execute_reply": "2022-05-27T19:03:37.507297Z",
     "shell.execute_reply.started": "2022-05-27T19:03:02.147058Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import json\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "!pip install emoji --upgrade\n",
    "import emoji\n",
    "\n",
    "!pip install tweet-preprocessor\n",
    "import preprocessor as p\n",
    "\n",
    "# !pip install -U spacy\n",
    "# !pip install texthero\n",
    "# import texthero as hero\n",
    "\n",
    "!pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is using datasets provided by [Bwandowando on Kaggle](https://www.kaggle.com/datasets/bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:03:37.510053Z",
     "iopub.status.busy": "2022-05-27T19:03:37.509816Z",
     "iopub.status.idle": "2022-05-27T19:03:37.531669Z",
     "shell.execute_reply": "2022-05-27T19:03:37.531047Z",
     "shell.execute_reply.started": "2022-05-27T19:03:37.510021Z"
    }
   },
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        full_path=os.path.join(dirname, filename)\n",
    "        all_files.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:03:37.53308Z",
     "iopub.status.busy": "2022-05-27T19:03:37.532774Z",
     "iopub.status.idle": "2022-05-27T19:03:37.536961Z",
     "shell.execute_reply": "2022-05-27T19:03:37.53624Z",
     "shell.execute_reply.started": "2022-05-27T19:03:37.533044Z"
    }
   },
   "outputs": [],
   "source": [
    "# sort the files\n",
    "all_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we will only look at February data. Seeing the filenames above and using regex, we can grab only the February files. Subsequent notebooks will cover the remaining months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:03:37.540383Z",
     "iopub.status.busy": "2022-05-27T19:03:37.539872Z",
     "iopub.status.idle": "2022-05-27T19:03:37.547647Z",
     "shell.execute_reply": "2022-05-27T19:03:37.546809Z",
     "shell.execute_reply.started": "2022-05-27T19:03:37.540349Z"
    }
   },
   "outputs": [],
   "source": [
    "# fetch all February files - filenames containing \"FEB\" or \"202202\"\n",
    "feb_files = [file for file in all_files if re.search(r\"FEB\", file) or re.search(r\"202202\", file)]\n",
    "feb_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the files and concatenate them into one pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:03:37.549761Z",
     "iopub.status.busy": "2022-05-27T19:03:37.549308Z",
     "iopub.status.idle": "2022-05-27T19:04:11.704623Z",
     "shell.execute_reply": "2022-05-27T19:04:11.703838Z",
     "shell.execute_reply.started": "2022-05-27T19:03:37.549725Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_df_list = []\n",
    "for file in feb_files:\n",
    "    print(f\"Reading in {file}\")\n",
    "    # unzip and read in the csv file as a dataframe\n",
    "    tmp_df = pd.read_csv(file, compression=\"gzip\", header=0, index_col=0)\n",
    "    # append dataframe to temp list\n",
    "    tmp_df_list.append(tmp_df)\n",
    "\n",
    "print(\"Concatenating the DataFrames\")\n",
    "# concatenate the dataframes in the temp list row-wise\n",
    "feb_df= pd.concat(tmp_df_list, axis=0)\n",
    "print(\"Concatenation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:11.706437Z",
     "iopub.status.busy": "2022-05-27T19:04:11.705806Z",
     "iopub.status.idle": "2022-05-27T19:04:11.728134Z",
     "shell.execute_reply": "2022-05-27T19:04:11.727371Z",
     "shell.execute_reply.started": "2022-05-27T19:04:11.706397Z"
    }
   },
   "outputs": [],
   "source": [
    "# show the first 5 rows of the february dataframe\n",
    "feb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:11.730128Z",
     "iopub.status.busy": "2022-05-27T19:04:11.729794Z",
     "iopub.status.idle": "2022-05-27T19:04:11.73521Z",
     "shell.execute_reply": "2022-05-27T19:04:11.734377Z",
     "shell.execute_reply.started": "2022-05-27T19:04:11.730086Z"
    }
   },
   "outputs": [],
   "source": [
    "# get shape of the DataFrame\n",
    "print(f\"{feb_df.shape[0]} rows and {feb_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:11.737189Z",
     "iopub.status.busy": "2022-05-27T19:04:11.736781Z",
     "iopub.status.idle": "2022-05-27T19:04:11.755143Z",
     "shell.execute_reply": "2022-05-27T19:04:11.754442Z",
     "shell.execute_reply.started": "2022-05-27T19:04:11.737153Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `userid`: unique number given to each user\n",
    "- `username`: user-defined name on Twitter (e.g., @johndoe)\n",
    "- `acctdesc`: user-made account description\n",
    "- `location`: user-defined location information (e.g., where they are based)\n",
    "- `following`: number of users the author of the tweet is following\n",
    "- `followers`: number of users following the author of the tweet\n",
    "- `totaltweets`: total number of tweets by the author of the tweet\n",
    "- `usercreatedts`: timestamp of when the user account was created\n",
    "- `tweetid` unique number given to each tweet\n",
    "- `tweetcreatedts`: timestamp of when the tweet was made\n",
    "- `retweetcount`: number of times the tweet was retweeted\n",
    "- `text`: text/content of the tweet\n",
    "- `hashtags`: hashtags in the tweet\n",
    "- `language`: language code of the tweet\n",
    "- `coordinates`: user-defined coordinates at the time of tweeting\n",
    "- `favorite_count`: numbere of times the tweet was favorited\n",
    "- `extractedts`: timestamp of when the tweet was extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the dtypes of `usercreatedts`, `tweetcreatedts`, and `extractedts` to `datetime64` for easier operation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:11.756565Z",
     "iopub.status.busy": "2022-05-27T19:04:11.756294Z",
     "iopub.status.idle": "2022-05-27T19:04:13.297095Z",
     "shell.execute_reply": "2022-05-27T19:04:13.296363Z",
     "shell.execute_reply.started": "2022-05-27T19:04:11.756533Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df[\"usercreatedts\"] = pd.to_datetime(feb_df[\"usercreatedts\"])\n",
    "feb_df[\"tweetcreatedts\"] = pd.to_datetime(feb_df[\"tweetcreatedts\"])\n",
    "feb_df[\"extractedts\"] = pd.to_datetime(feb_df[\"extractedts\"])\n",
    "\n",
    "# check dtypes\n",
    "feb_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When were the earliest and latest tweets in this dataset created?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:13.300699Z",
     "iopub.status.busy": "2022-05-27T19:04:13.300101Z",
     "iopub.status.idle": "2022-05-27T19:04:13.318417Z",
     "shell.execute_reply": "2022-05-27T19:04:13.317759Z",
     "shell.execute_reply.started": "2022-05-27T19:04:13.300671Z"
    }
   },
   "outputs": [],
   "source": [
    "earliest_tweet = feb_df[\"tweetcreatedts\"].min()\n",
    "latest_tweet = feb_df[\"tweetcreatedts\"].max()\n",
    "\n",
    "print(f\"The earliest tweet was at {earliest_tweet}, and the latest was at {latest_tweet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that the war began at early hours of the 24th of February, the earliest tweet in this dataset came only a few hours after. The latest tweet in this dataset came about half an hour before the end of February. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize tweet frequency by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:13.319929Z",
     "iopub.status.busy": "2022-05-27T19:04:13.319677Z",
     "iopub.status.idle": "2022-05-27T19:04:13.793608Z",
     "shell.execute_reply": "2022-05-27T19:04:13.792951Z",
     "shell.execute_reply.started": "2022-05-27T19:04:13.319896Z"
    }
   },
   "outputs": [],
   "source": [
    "# get dates in the dataframe \n",
    "dates = feb_df[\"tweetcreatedts\"].dt.day\n",
    "# group tweet timestamps by date and get tweet count for each date\n",
    "tweetcount_by_date = feb_df[\"tweetcreatedts\"].groupby(dates).size()\n",
    "\n",
    "# plot bar graph of tweet count by date\n",
    "tweetcount_by_date.plot.bar();\n",
    "\n",
    "plt.title(\"February Tweet Count by Date\")\n",
    "plt.xlabel(\"Tweet Date\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the month of February, the number of tweets about the war in Ukraine peaked on the 27th, 3 days after the start of the conflict. We can see that number of tweets jumped on the second day to over 400,000 tweets from 300,000 on the first day and stayed that way until decreasing on the 28th. At this point it is difficult to reason why the number went down on the 28th. However, it is clear that more people started to talk about the war starting the second day (the 25th)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many languages are in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:13.79521Z",
     "iopub.status.busy": "2022-05-27T19:04:13.794942Z",
     "iopub.status.idle": "2022-05-27T19:04:14.042625Z",
     "shell.execute_reply": "2022-05-27T19:04:14.04182Z",
     "shell.execute_reply.started": "2022-05-27T19:04:13.795158Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"There are {feb_df['language'].nunique()} unique languages in this DataFrame.\")\n",
    "feb_df[\"language\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of the tweets is in English (en)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:14.044339Z",
     "iopub.status.busy": "2022-05-27T19:04:14.044065Z",
     "iopub.status.idle": "2022-05-27T19:04:15.172619Z",
     "shell.execute_reply": "2022-05-27T19:04:15.171704Z",
     "shell.execute_reply.started": "2022-05-27T19:04:14.044292Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{round(feb_df.loc[feb_df['language']=='en'].shape[0]/feb_df.shape[0]*100, 2)}% of the tweets are in English.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:15.174391Z",
     "iopub.status.busy": "2022-05-27T19:04:15.174119Z",
     "iopub.status.idle": "2022-05-27T19:04:15.569014Z",
     "shell.execute_reply": "2022-05-27T19:04:15.568178Z",
     "shell.execute_reply.started": "2022-05-27T19:04:15.174356Z"
    }
   },
   "outputs": [],
   "source": [
    "language_counts = feb_df.groupby(\"language\").size().sort_values(ascending=False)[0:20].plot.bar(figsize=(12,6),\n",
    "                                                                                         title=\"Top 20 Languages by Frequency\",\n",
    "                                                                                         xlabel=\"Language Code\",\n",
    "                                                                                         ylabel=\"Number of Tweets\",\n",
    "                                                                                         rot=90\n",
    "                                                                                         );\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that English (en) was by far the most prevalent language in this dataset, nearing 1.2 million tweets out of 1.96 million. The second and third most prevalent languages were French and Thai, respectively. \n",
    "\n",
    "Note that the sixth most prevalent language was \"und\", which is used to indicate that Twitter could not detect a language. Let's inspect these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:15.570691Z",
     "iopub.status.busy": "2022-05-27T19:04:15.57044Z",
     "iopub.status.idle": "2022-05-27T19:04:15.864575Z",
     "shell.execute_reply": "2022-05-27T19:04:15.863757Z",
     "shell.execute_reply.started": "2022-05-27T19:04:15.570657Z"
    }
   },
   "outputs": [],
   "source": [
    "# pull the rows for which their language code is \"und\" or undefined\n",
    "language_und = feb_df.loc[feb_df[\"language\"]==\"und\"]\n",
    "# show full length of the text without truncating (...)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# show tweets\n",
    "language_und[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tweets appear to have lots of hashtags but little text. There are a couple exceptions, however. For example, there is a tweet that is in Croatian (\"Hm… sve što se današ u Ukrajini...\") ending with a hashtag in English (\"#Ukraine\"). Another tweet has both Ukrainian and English. Perhaps, Twitter defaults to assigning \"und\" when a tweet contains more than one language. Regardless, we are going limit the scope of the project to English tweets, which we will implement in code soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of English tweets jumped after the first day of the war to over 400,000 and stayed that way through the third day, the 26th, compared to slightly fewer thann 300,000 tweets on the first day of the conflict, or the 24th. After the third day, however, the number slightly went down on the 27th and then rebounded on the 28th. The decrease on the 27th could be explained by many factors. For one, it could be that the tweet extraction process could have stared late or got cut early. We could get to the bottom of this by checking the earliest and latest tweet timestamps of each day in the DataFrame. Or, the number could have dropped for other reasons that are nebulous to us right now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the earliest and latest `tweetcreatedts` timestamps of each day and calculate the time difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:15.866333Z",
     "iopub.status.busy": "2022-05-27T19:04:15.866049Z",
     "iopub.status.idle": "2022-05-27T19:04:16.231763Z",
     "shell.execute_reply": "2022-05-27T19:04:16.231042Z",
     "shell.execute_reply.started": "2022-05-27T19:04:15.866285Z"
    }
   },
   "outputs": [],
   "source": [
    "earliest_tweetts = feb_df[\"tweetcreatedts\"].groupby(feb_df[\"tweetcreatedts\"].dt.day).min()\n",
    "latest_tweetts = feb_df[\"tweetcreatedts\"].groupby(feb_df[\"tweetcreatedts\"].dt.day).max()\n",
    "\n",
    "print(f\"Earliest tweet timestamp of each day: {earliest_tweetts} \\n\")\n",
    "print(f\"Latest tweet timestamp of each day: {latest_tweetts} \\n\")\n",
    "\n",
    "print(\"Timespan between first tweet of the day and the last tweet for each day shown below:\")\n",
    "# calculate the timespan between first tweet of the day and the last tweet\n",
    "latest_tweetts - earliest_tweetts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first day had the shortest timespan of just over 17 hours between the first tweet collected and the last one. That is because the earliest timestamp for that day was at 06:48 in the morning, whereas for other days the earliest timestamps were just after midnight (00:00). \n",
    "\n",
    "Upon calculating the time differences, we can see that, as expected, the first and last tweet on the 24th had the shortest span of 17 hours and 11 minutes. The second shortest day was the 28th with 23 hours and 24 minutes between its first and last tweets. The 27th was the third shortest day with 23 hours and 50 minutes, but it was only 5-6 minutes shorter than the two longest days, the 25th and 26th. Although we cannot say for certain that the times between the first and last tweets could explain the decreases on the 27th and 28th, they seem to be contributing factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:16.233803Z",
     "iopub.status.busy": "2022-05-27T19:04:16.233299Z",
     "iopub.status.idle": "2022-05-27T19:04:18.970829Z",
     "shell.execute_reply": "2022-05-27T19:04:18.970033Z",
     "shell.execute_reply.started": "2022-05-27T19:04:16.233766Z"
    }
   },
   "outputs": [],
   "source": [
    "min_len = feb_df[\"text\"].str.len().min()\n",
    "max_len = feb_df[\"text\"].str.len().max()\n",
    "\n",
    "\n",
    "print(f\"Shortest tweet has {min_len} chars.\")\n",
    "print(f\"Longest tweet has {max_len} chars.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold on, a tweet can have 280 characters max. How could one have more than the limit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:18.972645Z",
     "iopub.status.busy": "2022-05-27T19:04:18.971956Z",
     "iopub.status.idle": "2022-05-27T19:04:20.084148Z",
     "shell.execute_reply": "2022-05-27T19:04:20.083164Z",
     "shell.execute_reply.started": "2022-05-27T19:04:18.972606Z"
    }
   },
   "outputs": [],
   "source": [
    "# get index of the tweet that has the max length\n",
    "max_len_index = feb_df[\"text\"].str.len().idxmax()\n",
    "# pull out the text of that index\n",
    "feb_df.loc[max_len_index, \"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon research, mentions supposedly do not count toward the character limit when the tweet is a reply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distribution of tweet lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:20.085886Z",
     "iopub.status.busy": "2022-05-27T19:04:20.085515Z",
     "iopub.status.idle": "2022-05-27T19:04:21.859779Z",
     "shell.execute_reply": "2022-05-27T19:04:21.858371Z",
     "shell.execute_reply.started": "2022-05-27T19:04:20.085843Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_len_series = feb_df[\"text\"].str.len()\n",
    "tweet_len_series.plot.hist();\n",
    "plt.title(\"Distribution of Tweet Length\")\n",
    "plt.xlabel(\"Tweet Length (Characters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# draw a vertical line for the mean\n",
    "plt.axvline(x=tweet_len_series.mean(), color=\"red\")\n",
    "# draw a vertical line for the median\n",
    "plt.axvline(x=tweet_len_series.median(), color=\"yellow\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean: {tweet_len_series.mean()} chars\")\n",
    "print(f\"Median: {tweet_len_series.median()} chars\")\n",
    "print(f\"Standard deviation: {tweet_len_series.std()} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is right-skewed. Most tweets appear to be below 300 characers in length. But because we have a few outlying tweets that have anomalously long lengths, as investigated above, the histogram has an elongated x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the top 20 users by tweet frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:21.861482Z",
     "iopub.status.busy": "2022-05-27T19:04:21.861209Z",
     "iopub.status.idle": "2022-05-27T19:04:23.669223Z",
     "shell.execute_reply": "2022-05-27T19:04:23.668159Z",
     "shell.execute_reply.started": "2022-05-27T19:04:21.861447Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "feb_df[\"username\"].value_counts().sort_values(ascending=True)[-20:].plot.barh();\n",
    "plt.title(\"Top 20 Users by Tweet Frequency\")\n",
    "plt.xlabel(\"Tweet Frequency\")\n",
    "plt.ylabel(\"Username\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 20 users tweeted hundreds of times during the last 4 days of February. Such frequencies raise suspicion that they are bots or institution-operated, like media companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate user account ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:23.674397Z",
     "iopub.status.busy": "2022-05-27T19:04:23.673973Z",
     "iopub.status.idle": "2022-05-27T19:04:23.689678Z",
     "shell.execute_reply": "2022-05-27T19:04:23.688462Z",
     "shell.execute_reply.started": "2022-05-27T19:04:23.674357Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:23.695385Z",
     "iopub.status.busy": "2022-05-27T19:04:23.694982Z",
     "iopub.status.idle": "2022-05-27T19:04:23.719662Z",
     "shell.execute_reply": "2022-05-27T19:04:23.718911Z",
     "shell.execute_reply.started": "2022-05-27T19:04:23.695251Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df[[\"usercreatedts\", \"extractedts\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:23.721448Z",
     "iopub.status.busy": "2022-05-27T19:04:23.720985Z",
     "iopub.status.idle": "2022-05-27T19:04:23.764545Z",
     "shell.execute_reply": "2022-05-27T19:04:23.76364Z",
     "shell.execute_reply.started": "2022-05-27T19:04:23.721411Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df[\"account_age\"] = (feb_df[\"extractedts\"]-feb_df[\"usercreatedts\"])\n",
    "# sns.histplot(feb_df[\"account_age\"])\n",
    "# plt.xlimit()\n",
    "# feb_df[\"account_age\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:23.766653Z",
     "iopub.status.busy": "2022-05-27T19:04:23.766073Z",
     "iopub.status.idle": "2022-05-27T19:04:23.808608Z",
     "shell.execute_reply": "2022-05-27T19:04:23.803385Z",
     "shell.execute_reply.started": "2022-05-27T19:04:23.76657Z"
    }
   },
   "outputs": [],
   "source": [
    "idxmin = feb_df[\"usercreatedts\"].idxmin()\n",
    "feb_df.loc[idxmin,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:23.811378Z",
     "iopub.status.busy": "2022-05-27T19:04:23.811136Z",
     "iopub.status.idle": "2022-05-27T19:04:23.827454Z",
     "shell.execute_reply": "2022-05-27T19:04:23.826564Z",
     "shell.execute_reply.started": "2022-05-27T19:04:23.811349Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df[\"account_age\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:23.830753Z",
     "iopub.status.busy": "2022-05-27T19:04:23.830118Z",
     "iopub.status.idle": "2022-05-27T19:04:23.855221Z",
     "shell.execute_reply": "2022-05-27T19:04:23.854269Z",
     "shell.execute_reply.started": "2022-05-27T19:04:23.830719Z"
    }
   },
   "outputs": [],
   "source": [
    "print(feb_df[\"account_age\"].min())\n",
    "print(feb_df[\"account_age\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which columns have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:23.860084Z",
     "iopub.status.busy": "2022-05-27T19:04:23.859753Z",
     "iopub.status.idle": "2022-05-27T19:04:25.117774Z",
     "shell.execute_reply": "2022-05-27T19:04:25.117118Z",
     "shell.execute_reply.started": "2022-05-27T19:04:23.860047Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`acctdesc` (account description), `location`, `coordinates` columns have missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`acctdesc` column contains account descriptions that users share on their Twitter profiles. At the moment, we are not concerned with such information. For now, we will rely on the tweets to learn more about what kind of words are frequently used and the users' sentiments surrounding the war in Ukraine. Therefore, we will drop `acctdesc` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:25.122428Z",
     "iopub.status.busy": "2022-05-27T19:04:25.121849Z",
     "iopub.status.idle": "2022-05-27T19:04:25.328516Z",
     "shell.execute_reply": "2022-05-27T19:04:25.327744Z",
     "shell.execute_reply.started": "2022-05-27T19:04:25.122388Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop acctdesc column\n",
    "feb_df.drop(\"acctdesc\", axis=1, inplace=True)\n",
    "# confirm it has been dropped\n",
    "feb_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many rows in the `location` column are missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:25.330446Z",
     "iopub.status.busy": "2022-05-27T19:04:25.329612Z",
     "iopub.status.idle": "2022-05-27T19:04:25.640041Z",
     "shell.execute_reply": "2022-05-27T19:04:25.639282Z",
     "shell.execute_reply.started": "2022-05-27T19:04:25.330408Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the number of rows missing location info\n",
    "missing_location_count = feb_df.loc[feb_df[\"location\"].isna()].shape[0]\n",
    "print(f\"{missing_location_count} rows are missing location information.\")\n",
    "print(f\"{round(missing_location_count/feb_df.shape[0]*100,2)}% of the rows are missing location information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many rows in `coordinates` column are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:25.641932Z",
     "iopub.status.busy": "2022-05-27T19:04:25.641246Z",
     "iopub.status.idle": "2022-05-27T19:04:26.042674Z",
     "shell.execute_reply": "2022-05-27T19:04:26.041181Z",
     "shell.execute_reply.started": "2022-05-27T19:04:25.641894Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_coordinates_count = feb_df.loc[feb_df[\"coordinates\"].isna()].shape[0]\n",
    "pct_missing_coordinates = round(missing_coordinates_count/feb_df.shape[0]*100,2)\n",
    "print(f\"{missing_coordinates_count} rows are missing location information.\")\n",
    "print(f\"{pct_missing_coordinates}% of the rows are missing coordinates.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of rows missing **both location and coordinates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:26.044671Z",
     "iopub.status.busy": "2022-05-27T19:04:26.043944Z",
     "iopub.status.idle": "2022-05-27T19:04:26.402573Z",
     "shell.execute_reply": "2022-05-27T19:04:26.401154Z",
     "shell.execute_reply.started": "2022-05-27T19:04:26.044633Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_location_coord = feb_df.loc[(feb_df[\"location\"].isna()) & \\\n",
    "                                   (feb_df[\"coordinates\"].isna())]\n",
    "print(f\"{missing_location_coord.shape[0]} rows, or {missing_location_coord.shape[0]/feb_df.shape[0]*100}%, \\\n",
    "are missing both location and coordinates data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a significant portion of the data is missing both location and coordinates information. This makes it difficult to impute the missing data. Given the current scope of the project, we will drop `location` and `coordinates` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:26.404083Z",
     "iopub.status.busy": "2022-05-27T19:04:26.403847Z",
     "iopub.status.idle": "2022-05-27T19:04:26.626829Z",
     "shell.execute_reply": "2022-05-27T19:04:26.626Z",
     "shell.execute_reply.started": "2022-05-27T19:04:26.40405Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df.drop([\"location\", \"coordinates\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the columns have been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:26.630215Z",
     "iopub.status.busy": "2022-05-27T19:04:26.630007Z",
     "iopub.status.idle": "2022-05-27T19:04:26.638981Z",
     "shell.execute_reply": "2022-05-27T19:04:26.638234Z",
     "shell.execute_reply.started": "2022-05-27T19:04:26.63019Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:26.641836Z",
     "iopub.status.busy": "2022-05-27T19:04:26.640118Z",
     "iopub.status.idle": "2022-05-27T19:04:27.100727Z",
     "shell.execute_reply": "2022-05-27T19:04:27.099993Z",
     "shell.execute_reply.started": "2022-05-27T19:04:26.6418Z"
    }
   },
   "outputs": [],
   "source": [
    "# pull duplicated rows based on tweetid column because tweetid is unique to each tweet\n",
    "# theoretically, there shouldn't be dupcliate tweetids; otherwise, we remove such duplicate rows\n",
    "# we sort values to display the duplicate tweets next to each other\n",
    "feb_df.loc[feb_df.duplicated([\"tweetid\"],keep=False)].sort_values(\"tweetid\").head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:27.102507Z",
     "iopub.status.busy": "2022-05-27T19:04:27.102207Z",
     "iopub.status.idle": "2022-05-27T19:04:27.722507Z",
     "shell.execute_reply": "2022-05-27T19:04:27.721693Z",
     "shell.execute_reply.started": "2022-05-27T19:04:27.102471Z"
    }
   },
   "outputs": [],
   "source": [
    "# by default, keep the first instance of the duplicates and drop the rest\n",
    "feb_df.drop_duplicates([\"tweetid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any duplicates remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:27.724124Z",
     "iopub.status.busy": "2022-05-27T19:04:27.723834Z",
     "iopub.status.idle": "2022-05-27T19:04:28.059611Z",
     "shell.execute_reply": "2022-05-27T19:04:28.058729Z",
     "shell.execute_reply.started": "2022-05-27T19:04:27.724087Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df.duplicated([\"tweetid\"]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:28.061255Z",
     "iopub.status.busy": "2022-05-27T19:04:28.060922Z",
     "iopub.status.idle": "2022-05-27T19:04:33.350796Z",
     "shell.execute_reply": "2022-05-27T19:04:33.350111Z",
     "shell.execute_reply.started": "2022-05-27T19:04:28.061217Z"
    }
   },
   "outputs": [],
   "source": [
    "# double check\n",
    "feb_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All duplicates have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T06:00:02.968029Z",
     "iopub.status.busy": "2022-05-04T06:00:02.967753Z",
     "iopub.status.idle": "2022-05-04T06:00:02.980699Z",
     "shell.execute_reply": "2022-05-04T06:00:02.979811Z",
     "shell.execute_reply.started": "2022-05-04T06:00:02.968Z"
    }
   },
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets can contain a lot of miscellaneous information when it comes to sentiment analysis. One example would be URLs: they don't help us gauge the sentiment. Irregularities such as uppercase and lowercase letters are typically unified into all lowercase letters by convention, though exceptions are occasionally taken. Mentions and hashtags are also removed. Smileys and emojis can be useful, but we will remove them for simplicity and agility for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what the text data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:33.35269Z",
     "iopub.status.busy": "2022-05-27T19:04:33.352218Z",
     "iopub.status.idle": "2022-05-27T19:04:33.360288Z",
     "shell.execute_reply": "2022-05-27T19:04:33.359384Z",
     "shell.execute_reply.started": "2022-05-27T19:04:33.352655Z"
    }
   },
   "outputs": [],
   "source": [
    "# show first 10 tweets\n",
    "feb_df[\"text\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we found before, there are multiple languages. For this project, we will only look at English tweets. Future direction of the project includes doing exercises for multi-lingual natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:33.362066Z",
     "iopub.status.busy": "2022-05-27T19:04:33.361794Z",
     "iopub.status.idle": "2022-05-27T19:04:33.808068Z",
     "shell.execute_reply": "2022-05-27T19:04:33.807304Z",
     "shell.execute_reply.started": "2022-05-27T19:04:33.362012Z"
    }
   },
   "outputs": [],
   "source": [
    "# select only the rows whose tweets are in English\n",
    "feb_df = feb_df.loc[feb_df[\"language\"]==\"en\"]\n",
    "print(f\"{feb_df.shape[0]} rows are in English\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T06:07:22.32859Z",
     "iopub.status.busy": "2022-05-04T06:07:22.328294Z",
     "iopub.status.idle": "2022-05-04T06:07:22.346093Z",
     "shell.execute_reply": "2022-05-04T06:07:22.345155Z",
     "shell.execute_reply.started": "2022-05-04T06:07:22.328561Z"
    }
   },
   "source": [
    "Reset index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:33.810251Z",
     "iopub.status.busy": "2022-05-27T19:04:33.809333Z",
     "iopub.status.idle": "2022-05-27T19:04:33.828328Z",
     "shell.execute_reply": "2022-05-27T19:04:33.827523Z",
     "shell.execute_reply.started": "2022-05-27T19:04:33.810212Z"
    }
   },
   "outputs": [],
   "source": [
    "# reset index\n",
    "feb_df.reset_index(inplace=True, drop=True)\n",
    "# check\n",
    "feb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the text data one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:33.830133Z",
     "iopub.status.busy": "2022-05-27T19:04:33.829591Z",
     "iopub.status.idle": "2022-05-27T19:04:33.839574Z",
     "shell.execute_reply": "2022-05-27T19:04:33.838727Z",
     "shell.execute_reply.started": "2022-05-27T19:04:33.830091Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df[\"text\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts have URLs, emojis, mentions, hashtags, and HTML artifacts (e.g., \\n). Uppercase and lowercase letters are also mixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lowercase everything\n",
    "2. Remove URLs and HTML artifacts (e.g., &amp, \\n), hashtags, mentions, digits, and emojis\n",
    "4. Remove punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lowercase Everything\n",
    "Convert everythinng to lowercase.\n",
    "\n",
    "*Note: we could use [Texthero](https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db) to speed up this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:33.841609Z",
     "iopub.status.busy": "2022-05-27T19:04:33.840688Z",
     "iopub.status.idle": "2022-05-27T19:04:35.160935Z",
     "shell.execute_reply": "2022-05-27T19:04:35.160223Z",
     "shell.execute_reply.started": "2022-05-27T19:04:33.841559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lowercase everything\n",
    "feb_df[\"cleaned_text\"] = feb_df[\"text\"].str.lower()\n",
    "# check\n",
    "feb_df[\"cleaned_text\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything has been lowercased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove URLs, HTML, Hashtags, Mentions, Digits, and Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:04:35.16255Z",
     "iopub.status.busy": "2022-05-27T19:04:35.162184Z",
     "iopub.status.idle": "2022-05-27T19:08:16.669095Z",
     "shell.execute_reply": "2022-05-27T19:08:16.668043Z",
     "shell.execute_reply.started": "2022-05-27T19:04:35.162509Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_unnecessary(text):\n",
    "    # INPUT: string (tweet)\n",
    "    # OUTPUT: string without URLs, mentions, hashtags, digist, and emojis (and smileys)\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.NUMBER, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "    result = p.clean(text)\n",
    "    return result\n",
    "\n",
    "feb_df[\"cleaned_text\"] = feb_df[\"cleaned_text\"].map(remove_unnecessary)\n",
    "# check\n",
    "feb_df[\"cleaned_text\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:08:16.671379Z",
     "iopub.status.busy": "2022-05-27T19:08:16.670452Z",
     "iopub.status.idle": "2022-05-27T19:08:17.005858Z",
     "shell.execute_reply": "2022-05-27T19:08:17.005154Z",
     "shell.execute_reply.started": "2022-05-27T19:08:16.671342Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_set = set(STOPWORDS)\n",
    "wordcloud = WordCloud(background_color='white',\n",
    "                     stopwords = stopwords_set,\n",
    "                      max_words = 300,\n",
    "                      max_font_size = 40,\n",
    "                      scale = 2,\n",
    "                      random_state=42\n",
    "                     ).generate(str(feb_df['cleaned_text']))\n",
    "\n",
    "print(wordcloud)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove HTML Entities\n",
    "\n",
    "As for removing HTML entities, the most frequent ones are \"&amp\" and \"\\n\". We will replace these with empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:08:17.007014Z",
     "iopub.status.busy": "2022-05-27T19:08:17.006805Z",
     "iopub.status.idle": "2022-05-27T19:08:18.034124Z",
     "shell.execute_reply": "2022-05-27T19:08:18.033331Z",
     "shell.execute_reply.started": "2022-05-27T19:08:17.006987Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace \"&amp\" in tweets with empty string\n",
    "feb_df[\"cleaned_text\"] = feb_df[\"cleaned_text\"].str.replace(\"&amp\", \"\")\n",
    "\n",
    "# replace \"\\n\" in tweets with empty string\n",
    "# may not be necessary after applying remove_unnecessary func\n",
    "# feb_df[\"cleaned_text\"] = feb_df[\"cleaned_text\"].str.replace(\"\\n\", \"\")\n",
    "\n",
    "# check\n",
    "feb_df[\"cleaned_text\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:08:18.035788Z",
     "iopub.status.busy": "2022-05-27T19:08:18.035471Z",
     "iopub.status.idle": "2022-05-27T19:08:25.704916Z",
     "shell.execute_reply": "2022-05-27T19:08:25.704145Z",
     "shell.execute_reply.started": "2022-05-27T19:08:18.035751Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove punctuations using regex\n",
    "# reference: https://stackoverflow.com/questions/68641923/remove-puncts-from-pandas-dataframe\n",
    "feb_df[\"cleaned_text\"] = feb_df['cleaned_text'].str.replace(r'[^0-9a-zA-Z\\s]+', '', regex=True)\n",
    "\n",
    "# check\n",
    "feb_df[\"cleaned_text\"].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuations have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this DataFrame with `cleaned_text` as a pickle file for fast loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:08:25.707049Z",
     "iopub.status.busy": "2022-05-27T19:08:25.70655Z",
     "iopub.status.idle": "2022-05-27T19:08:27.470778Z",
     "shell.execute_reply": "2022-05-27T19:08:27.470003Z",
     "shell.execute_reply.started": "2022-05-27T19:08:25.70701Z"
    }
   },
   "outputs": [],
   "source": [
    "# save dataframea as a pickle file for later loading\n",
    "feb_df.to_pickle(\"feb_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:08:27.472138Z",
     "iopub.status.busy": "2022-05-27T19:08:27.471884Z",
     "iopub.status.idle": "2022-05-27T19:09:37.083875Z",
     "shell.execute_reply": "2022-05-27T19:09:37.083177Z",
     "shell.execute_reply.started": "2022-05-27T19:08:27.472104Z"
    }
   },
   "outputs": [],
   "source": [
    "# concatenate all tweets in cleaned_text column into one long string for wordcloud to accept\n",
    "text = \" \".join(tweet for tweet in feb_df[\"cleaned_text\"])\n",
    "\n",
    "# reference: https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
    "\n",
    "wc = WordCloud(width=800, height=400, max_words=300).generate(text)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, we see that words like \"ukraine\", \"war\", and \"russia\" are the most prominent in the wordcloud, or most frequently appearing in the tweets. We can also feel a sense of shock and awe from the bigram \"whole world\", perhaps conveying how this war has gripped the world's attention or how it has implicated many countries around the globe in one way or another. One peculiarity could be that \"russian people\" is a frequent bigram, while \"ukranian people\" is missing from the cloud. This could be simply due to text preprocessing. However, we know that Putin has publicly claimed that he is conducting this \"special military operation\" to save ethnic Russians in Ukraine who are allegedly being oppressed, so that could be a reason this bigram takes up a prominent space in the cloud. In fact, he has been saying and writing outlandish theories of his own about Ukraine for many years and of couse just prior to and during this war. Wordcloud is just an exploratory tool to see what words are pervasive in the data. We shall dive deeper into analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Using RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each tweet, the RoBERTa model will generate a score for each of negative, neutral, and positive sentiments.\n",
    "\n",
    "As this is my first time conducting sentiment analysis, following work heavily relies on [S Sai Suryateja's work](https://www.kaggle.com/code/ssaisuryateja/eda-and-sentiment-analysis/notebook#Sentiment-and-Emotion-Analysis). I did some research to understand the code line by line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:11:44.666377Z",
     "iopub.status.busy": "2022-05-27T19:11:44.666031Z",
     "iopub.status.idle": "2022-05-27T19:11:44.673091Z",
     "shell.execute_reply": "2022-05-27T19:11:44.672144Z",
     "shell.execute_reply.started": "2022-05-27T19:11:44.666297Z"
    }
   },
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/code/ssaisuryateja/eda-and-sentiment-analysis#EDA\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "# set device to cuda:0 if it's available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:12:17.829435Z",
     "iopub.status.busy": "2022-05-27T19:12:17.829105Z",
     "iopub.status.idle": "2022-05-27T19:12:45.68393Z",
     "shell.execute_reply": "2022-05-27T19:12:45.68321Z",
     "shell.execute_reply.started": "2022-05-27T19:12:17.829403Z"
    }
   },
   "outputs": [],
   "source": [
    "# get pretrained tokenizer from cardiffnlp repo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "# create instance of twitter-roberta-base-sentiment classification model and attach it to the cuda\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:13:39.343953Z",
     "iopub.status.busy": "2022-05-27T19:13:39.343187Z",
     "iopub.status.idle": "2022-05-27T19:13:39.631712Z",
     "shell.execute_reply": "2022-05-27T19:13:39.630784Z",
     "shell.execute_reply.started": "2022-05-27T19:13:39.343902Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import csv\n",
    "\n",
    "labels=[] # will contain 'positive', 'neutral', 'negative'\n",
    "task = 'sentiment' # our task is sentiment analysis\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:13:44.422229Z",
     "iopub.status.busy": "2022-05-27T19:13:44.421974Z",
     "iopub.status.idle": "2022-05-27T19:13:44.427569Z",
     "shell.execute_reply": "2022-05-27T19:13:44.426893Z",
     "shell.execute_reply.started": "2022-05-27T19:13:44.422201Z"
    }
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use three labels: \"negative\", \"neutral\", and \"positive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T19:54:43.225615Z",
     "iopub.status.busy": "2022-05-27T19:54:43.225302Z",
     "iopub.status.idle": "2022-05-27T20:33:28.999275Z",
     "shell.execute_reply": "2022-05-27T20:33:28.997994Z",
     "shell.execute_reply.started": "2022-05-27T19:54:43.225583Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 100 # number of tweets in a batch that will be passed into tokenizer\n",
    "\n",
    "scores_all = np.empty((0,len(labels)))\n",
    "# create list of all the tweets in the dataset\n",
    "text_all = feb_df['cleaned_text'].to_list()\n",
    "n = len(text_all) # same as number of tweets\n",
    "with torch.no_grad():\n",
    "    for start_idx in tqdm(range(0, n, BATCH_SIZE)):\n",
    "        end_idx = min(start_idx + BATCH_SIZE, n) \n",
    "        # reference: https://huggingface.co/docs/transformers/preprocessing\n",
    "        # tokenize the tweets in the batch, return pytorch ('pt') tensors\n",
    "        # some tweets are shorter than the uniform tensor length needed; padding adds 0's to maintain uniform tensor length\n",
    "        # some tweets are too long; truncation truncates input to maximum length accepted by model\n",
    "        encoded_input = tokenizer(text_all[start_idx:end_idx], return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        \n",
    "        # references: https://stackoverflow.com/questions/11315010/what-do-and-before-a-variable-name-mean-in-a-function-signature\n",
    "        # https://stackoverflow.com/questions/1419046/normal-arguments-vs-keyword-arguments/1419160#1419160\n",
    "        output = model(**encoded_input)\n",
    "        # convert pytorch tensor to numpy\n",
    "        scores = output[0].detach().cpu().numpy()\n",
    "        # \n",
    "        scores = softmax(scores, axis=1)\n",
    "        scores_all = np.concatenate((scores_all, scores), axis=0)\n",
    "        \n",
    "        # delete encoded_input, output, scores for next batch\n",
    "        del encoded_input, output, scores \n",
    "        # release all unoccupied cached mem \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output below is what `scores_all` looks like. Each row contains scores for negative, neutral, and positive sentiments. The higher the score, the more likely the tweet has that sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:33:51.700025Z",
     "iopub.status.busy": "2022-05-27T20:33:51.69977Z",
     "iopub.status.idle": "2022-05-27T20:33:51.706388Z",
     "shell.execute_reply": "2022-05-27T20:33:51.705529Z",
     "shell.execute_reply.started": "2022-05-27T20:33:51.699996Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the scores with the existing DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:39:37.35171Z",
     "iopub.status.busy": "2022-05-27T20:39:37.351144Z",
     "iopub.status.idle": "2022-05-27T20:39:37.39619Z",
     "shell.execute_reply": "2022-05-27T20:39:37.395362Z",
     "shell.execute_reply.started": "2022-05-27T20:39:37.351669Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df[labels] = pd.DataFrame(scores_all, columns=labels)\n",
    "feb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this DataFrame so that we don't have to run the model again, which takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:41:01.674077Z",
     "iopub.status.busy": "2022-05-27T20:41:01.673618Z",
     "iopub.status.idle": "2022-05-27T20:42:00.585478Z",
     "shell.execute_reply": "2022-05-27T20:42:00.584695Z",
     "shell.execute_reply.started": "2022-05-27T20:41:01.674037Z"
    }
   },
   "outputs": [],
   "source": [
    "feb_df.to_csv(\"./feb_sentiment_analysis_RoBERTa_raw_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[More interpretative work to come.]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
